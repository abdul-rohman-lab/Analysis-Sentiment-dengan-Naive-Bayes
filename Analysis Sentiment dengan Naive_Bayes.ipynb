{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17. Analysis Sentiment - Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UFOZtDjCHQF"
      },
      "source": [
        "# **Analysis Sentiment - Naive Bayes**\n",
        "\n",
        " \n",
        "\n",
        "Sekarang kita akan mencoba untuk melakukan Analsis sentiment menggunakan salah satu metode Supervised Learning yaitu Naive Bayes. Seperti yang kita tahu bahwa pada supervised learning kita membutuhkan dataset agar mesin kita dapat belajar jadi pastika kita mempunyai dataset sudah memiliki dataset tersebut. Bisa dengan cara melakukan analisis sentiment terlebih dahulu dengan menggunakan perbandingan antara kata negatif-positif seperti kemarin, atau dengan mencari dataset yang sudah diberi label di internet.\n",
        "\n",
        "Pada kesempatan ini kalian bisa menggunakan dataset tweet di bawah, untuk memperingan kinerja pada kesempatan pertama ini maka diberikan dataset dengan ukuran kecil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRp75YehCKae"
      },
      "source": [
        "ini untuk dataset tweet yang diguakan\n",
        "\n",
        "[dataset_tweet_2](https://blog.sanbercode.com/wp-content/uploads/2020/09/dataset_tweet_2.csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_p9GEaCT9Y"
      },
      "source": [
        "Mari kita coba baca dataset kita lalu membaginya kedalam dua atribut, yaitu tweet dan label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqn-CIMqBaH1",
        "outputId": "92906fc0-39a2-4f74-b425-9cf1dda2a4a6"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "file = 'dataset_tweet_2.csv'\n",
        "\n",
        "token_data = open(file)\n",
        "tokens = csv.reader(token_data, delimiter=';')\n",
        "tweets = []\n",
        "label = []\n",
        "for row in tokens:\n",
        "    tweets.append(row[0])\n",
        "    label.append(int(row[1].replace(',','')))\n",
        "\n",
        "df = pd.DataFrame(columns=['tweets','label'])\n",
        "df['tweets'] = tweets\n",
        "df['label'] = label\n",
        "\n",
        "print (df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               tweets  label\n",
            "0   rt @napqilla: no 1, 3 ambisinya menguasai raky...      1\n",
            "1   rt @pandji: nah gue pikir sentimen petahana ok...      1\n",
            "2   rt @pandji: urutan pertama best moment #debat2...      1\n",
            "3   rt @pandji: ini artikel yg menjelaskan ternyat...      1\n",
            "4   rt @mrtampi: agus makin santai.\\nahok makin sa...      0\n",
            "..                                                ...    ...\n",
            "76  rt @pandji: nah gue pikir sentimen petahana ok...      0\n",
            "77  rt @josua_tm: ibu sylvi adalah contoh bahwa wa...      1\n",
            "78  besok saya ajak kesana saja, saya udah survei ...      1\n",
            "79  benerr bgt.. dan tidak mengajak penonton ikut ...      1\n",
            "80  rt @gandy_koz: pak anis,kl pas libur lebaran i...      1\n",
            "\n",
            "[81 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA51-iW0D01d"
      },
      "source": [
        "lalu selanjutnya kita akan melakukan pembersihan tweet di atas, kita akan memanfaatkan modul stemming pada sastrawi, jadi jangan lupa untuk menambahkan import pada bagian library. pertama kita akan melakukan case folding lalu kita akan melakukan stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jktA9wogkowi",
        "outputId": "4fd95c5f-2037-41cd-d2fe-789430fcee1b"
      },
      "source": [
        "pip install sastrawi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4b/bab676953da3103003730b8fcdfadbdd20f333d4add10af949dd5c51e6ed/Sastrawi-1.0.1-py2.py3-none-any.whl (209kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 10.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122kB 11.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 174kB 11.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 184kB 11.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 194kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 204kB 11.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 11.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcel_O2BD2jM",
        "outputId": "d3887ea1-e04a-4401-c867-10986e26c795"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "\n",
        "import re,string\n",
        "\n",
        "clean_tweets = []\n",
        "for tweet in tweets:\n",
        "    def hapus_tanda(tweet): \n",
        "        tanda_baca = set(string.punctuation)\n",
        "        tweet = ''.join(ch for ch in tweet if ch not in tanda_baca)\n",
        "        return tweet\n",
        "    \n",
        "    tweet=tweet.lower()\n",
        "    tweet = re.sub(r'\\\\u\\w\\w\\w\\w', '', tweet)\n",
        "    tweet=re.sub(r'http\\S+','',tweet)\n",
        "    #hapus @username\n",
        "    tweet=re.sub('@[^\\s]+','',tweet)\n",
        "    #hapus #tagger \n",
        "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "    #hapus tanda baca\n",
        "    tweet=hapus_tanda(tweet)\n",
        "    #hapus angka dan angka yang berada dalam string \n",
        "    tweet=re.sub(r'\\w*\\d\\w*', '',tweet).strip()\n",
        "    \n",
        "    #stemming\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    tweet = stemmer.stem(tweet)\n",
        "    clean_tweets.append(tweet)\n",
        "\n",
        "df['clean'] = clean_tweets\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              tweets  ...                                              clean\n",
            "0  rt @napqilla: no 1, 3 ambisinya menguasai raky...  ...  rt no ambisi kuasa rakyat ambisi layan rakyat ...\n",
            "1  rt @pandji: nah gue pikir sentimen petahana ok...  ...  rt nah gue pikir sentimen tahana oke di malam ...\n",
            "2  rt @pandji: urutan pertama best moment #debat2...  ...  rt urut pertama best moment pak basuki misahin...\n",
            "3  rt @pandji: ini artikel yg menjelaskan ternyat...  ...  rt ini artikel yg jelas nyata di yg dapet resp...\n",
            "4  rt @mrtampi: agus makin santai.\\nahok makin sa...  ...  rt agus makin santainahok makin santunnanies m...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hw7zS-Mlg7J"
      },
      "source": [
        "# **Machine Learning & Data Teks**\n",
        "\n",
        "Seperti yang kita tahu untuk dapat menggunakan sebuah model machine learning maka kita membutuhkan data dengan tipe numerik atau integer, akan tetapi salah satu satu feature yang kita miliki merupakan data teks. Nah pada kesempatan ini kita akan memanfaatkan metode perubahan data teks - matriks yang sudah kita pelajari sebelumnya, yaitu TF-IDF\n",
        "\n",
        "Mari kita mulai, kita akan memanfaatkan metode TfidVectorizer pada library sklearn dan gaussian Naive Bayes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oQwYf5-ljKR"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "vectorizer = TfidfVectorizer (max_features=2500)\n",
        "model_g = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x69vY59hl7gP"
      },
      "source": [
        "Lalu kita ubah data clean_tweet kita ke dalam bentuk TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfF9myyQl4z5",
        "outputId": "b4d9fbd7-6405-4c37-f86e-22fc0d2f8ddf"
      },
      "source": [
        "v_data = vectorizer.fit_transform(df['clean']).toarray()\n",
        "\n",
        "print(v_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW-Np2YXmHh0",
        "outputId": "cf98b904-ac9d-47d5-9537-351140e7ef6a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(v_data, df['label'], test_size=0.2, random_state=0)\n",
        "model_g.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOPW3Wvckdeb",
        "outputId": "22f76c75-8e8d-41f5-bd73-75badc10b951"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "y_preds = model_g.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test,y_preds))\n",
        "print(classification_report(y_test,y_preds))\n",
        "print('nilai akurasinya adalah ',accuracy_score(y_test, y_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8 4]\n",
            " [1 4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.76        12\n",
            "           1       0.50      0.80      0.62         5\n",
            "\n",
            "    accuracy                           0.71        17\n",
            "   macro avg       0.69      0.73      0.69        17\n",
            "weighted avg       0.77      0.71      0.72        17\n",
            "\n",
            "nilai akurasinya adalah  0.7058823529411765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0JrFYhXnvcS"
      },
      "source": [
        "Untuk melakukan prediksi jangan lupa untuk mengubah 0 dan 1 menjadi sebuah parameter string pada akhir langkah."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVxWJVMfpaZn",
        "outputId": "5777efa7-99ca-46b9-a6f8-61e21d498849"
      },
      "source": [
        "y_preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOjHhav0nwUp",
        "outputId": "1b9d6930-c796-4076-e796-2ce8361abb77"
      },
      "source": [
        "tweet = ''\n",
        "v_data = vectorizer.transform([tweet]).toarray()\n",
        "y_preds = model_g.predict(v_data)\n",
        "\n",
        "# dengan asumsi bahwa 1 merupan label positif\n",
        "if y_preds == 1:\n",
        "  print('positif')\n",
        "else:\n",
        "  print('negatif')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positif\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}